{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiDAF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import argparse\n",
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--data_dir\", required=True)\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(out_file, line):\n",
    "    line = line+'\\n'\n",
    "    out_file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_from_json(filename):\n",
    "    \"\"\"Loads JSON data from filename and returns\"\"\"\n",
    "    with open(filename, encoding = 'utf-8') as data_file:\n",
    "        data = json.load(data_file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sequence):\n",
    "    tokens = [token.replace(\"``\", '\"').replace(\"''\", '\"').lower() for token in nltk.word_tokenize(sequence)]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_exs(dataset):\n",
    "    \"\"\"\n",
    "    Returns the total number of (context, question, answer) triples,\n",
    "    given the data read from the SQuAD json file.\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    for article in dataset['data']:\n",
    "        for para in article['paragraphs']:\n",
    "            total += len(para['qas'])\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reporthook(t):\n",
    "    \"\"\"https://github.com/tqdm/tqdm\"\"\"\n",
    "    last_b = [0]\n",
    "\n",
    "    def inner(b=1, bsize=1, tsize=None):\n",
    "        \"\"\"\n",
    "        b: int, optional\n",
    "            Number of blocks just transferred [default: 1].\n",
    "        bsize: int, optional\n",
    "            Size of each block (in tqdm units) [default: 1].\n",
    "        tsize: int, optional\n",
    "            Total size (in tqdm units). If [default: None] remains unchanged.\n",
    "        \"\"\"\n",
    "        if tsize is not None:\n",
    "            t.total = tsize\n",
    "        t.update((b - last_b[0]) * bsize)\n",
    "        last_b[0] = b\n",
    "\n",
    "    return inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_download(url, filename, prefix, num_bytes=None):\n",
    "    \"\"\"Takes an URL, a filename, and the expected bytes, download\n",
    "    the contents and returns the filename.\n",
    "    num_bytes=None disables the file size check.\"\"\"\n",
    "    local_filename = None\n",
    "    if not os.path.exists(os.path.join(prefix, filename)):\n",
    "        try:\n",
    "            print(\"Downloading file {}...\".format(url + filename))\n",
    "            with tqdm(unit='B', unit_scale=True, miniters=1, desc=filename) as t:\n",
    "                local_filename, _ = urlretrieve(url + filename, os.path.join(prefix, filename), reporthook=reporthook(t))\n",
    "        except AttributeError as e:\n",
    "            print(\"An error occurred when downloading the file! Please get the dataset using a browser.\")\n",
    "            raise e\n",
    "    # We have a downloaded file\n",
    "    # Check the stats and make sure they are ok\n",
    "    file_stats = os.stat(os.path.join(prefix, filename))\n",
    "    if num_bytes is None or file_stats.st_size == num_bytes:\n",
    "        print(\"File {} successfully loaded\".format(filename))\n",
    "    else:\n",
    "        raise Exception(\"Unexpected dataset size. Please get the dataset using a browser.\")\n",
    "\n",
    "    return local_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_char_word_loc_mapping(context, context_tokens):\n",
    "    \"\"\"\n",
    "    Return a mapping that maps from character locations to the corresponding token locations.\n",
    "    If we're unable to complete the mapping e.g. because of special characters, we return None.\n",
    "    Inputs:\n",
    "      context: string (unicode)\n",
    "      context_tokens: list of strings (unicode)\n",
    "    Returns:\n",
    "      mapping: dictionary from ints (character locations) to (token, token_idx) pairs\n",
    "        Only ints corresponding to non-space character locations are in the keys\n",
    "        e.g. if context = \"hello world\" and context_tokens = [\"hello\", \"world\"] then\n",
    "        0,1,2,3,4 are mapped to (\"hello\", 0) and 6,7,8,9,10 are mapped to (\"world\", 1)\n",
    "    \"\"\"\n",
    "    acc = '' # accumulator\n",
    "    current_token_idx = 0 # current word loc\n",
    "    mapping = dict()\n",
    "\n",
    "    for char_idx, char in enumerate(context): # step through original characters\n",
    "        if char != u' ' and char != u'\\n': # if it's not a space:\n",
    "            acc += char # add to accumulator\n",
    "            context_token = str(context_tokens[current_token_idx]) # current word token\n",
    "            if acc == context_token: # if the accumulator now matches the current word token\n",
    "                syn_start = char_idx - len(acc) + 1 # char loc of the start of this word\n",
    "                for char_loc in range(syn_start, char_idx+1):\n",
    "                    mapping[char_loc] = (acc, current_token_idx) # add to mapping\n",
    "                acc = '' # reset accumulator\n",
    "                current_token_idx += 1\n",
    "\n",
    "    if current_token_idx != len(context_tokens):\n",
    "        return None\n",
    "    else:\n",
    "        return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ('hello', 0), 1: ('hello', 0), 2: ('hello', 0), 3: ('hello', 0), 4: ('hello', 0), 6: ('world', 1), 7: ('world', 1), 8: ('world', 1), 9: ('world', 1), 10: ('world', 1)}\n"
     ]
    }
   ],
   "source": [
    "print(get_char_word_loc_mapping('hello world',['hello','world']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_write(dataset, tier, out_dir):\n",
    "    \"\"\"Reads the dataset, extracts context, question, answer, tokenizes them,\n",
    "    and calculates answer span in terms of token indices.\n",
    "    Note: due to tokenization issues, and the fact that the original answer\n",
    "    spans are given in terms of characters, some examples are discarded because\n",
    "    we cannot get a clean span in terms of tokens.\n",
    "    This function produces the {train/dev}.{context/question/answer/span} files.\n",
    "    Inputs:\n",
    "      dataset: read from JSON\n",
    "      tier: string (\"train\" or \"dev\")\n",
    "      out_dir: directory to write the preprocessed files\n",
    "    Returns:\n",
    "      the number of (context, question, answer) triples written to file by the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    num_exs = 0 # number of examples written to file\n",
    "    num_mappingprob, num_tokenprob, num_spanalignprob = 0, 0, 0\n",
    "    examples = []\n",
    "\n",
    "    for articles_id in tqdm(range(len(dataset['data'])), desc=\"Preprocessing {}\".format(tier)):\n",
    "\n",
    "        article_paragraphs = dataset['data'][articles_id]['paragraphs']\n",
    "        for pid in range(len(article_paragraphs)):\n",
    "\n",
    "            context = str(article_paragraphs[pid]['context']) # string\n",
    "\n",
    "            # The following replacements are suggested in the paper\n",
    "            # BidAF (Seo et al., 2016)\n",
    "            context = context.replace(\"''\", '\" ')\n",
    "            context = context.replace(\"``\", '\" ')\n",
    "\n",
    "            context_tokens = tokenize(context) # list of strings (lowercase)\n",
    "            context = context.lower()\n",
    "\n",
    "            qas = article_paragraphs[pid]['qas'] # list of questions\n",
    "\n",
    "            charloc2wordloc = get_char_word_loc_mapping(context, context_tokens) # charloc2wordloc maps the character location (int) of a context token to a pair giving (word (string), word loc (int)) of that token\n",
    "\n",
    "            if charloc2wordloc is None: # there was a problem\n",
    "                num_mappingprob += len(qas)\n",
    "                continue # skip this context example\n",
    "\n",
    "            # for each question, process the question and answer and write to file\n",
    "            for qn in qas:\n",
    "\n",
    "                # read the question text and tokenize\n",
    "                question = str(qn['question']) # string\n",
    "                question_tokens = tokenize(question) # list of strings\n",
    "\n",
    "                # of the three answers, just take the first\n",
    "                ans_text = str(qn['answers'][0]['text']).lower() # get the answer text\n",
    "                ans_start_charloc = qn['answers'][0]['answer_start'] # answer start loc (character count)\n",
    "                ans_end_charloc = ans_start_charloc + len(ans_text) # answer end loc (character count) (exclusive)\n",
    "\n",
    "                # Check that the provided character spans match the provided answer text\n",
    "                if context[ans_start_charloc:ans_end_charloc] != ans_text:\n",
    "                  # Sometimes this is misaligned, mostly because \"narrow builds\" of Python 2 interpret certain Unicode characters to have length 2 https://stackoverflow.com/questions/29109944/python-returns-length-of-2-for-single-unicode-character-string\n",
    "                  # We should upgrade to Python 3 next year!\n",
    "                  num_spanalignprob += 1\n",
    "                  continue\n",
    "\n",
    "                # get word locs for answer start and end (inclusive)\n",
    "                ans_start_wordloc = charloc2wordloc[ans_start_charloc][1] # answer start word loc\n",
    "                ans_end_wordloc = charloc2wordloc[ans_end_charloc-1][1] # answer end word loc\n",
    "                assert ans_start_wordloc <= ans_end_wordloc\n",
    "\n",
    "                # Check retrieved answer tokens match the provided answer text.\n",
    "                # Sometimes they won't match, e.g. if the context contains the phrase \"fifth-generation\"\n",
    "                # and the answer character span is around \"generation\",\n",
    "                # but the tokenizer regards \"fifth-generation\" as a single token.\n",
    "                # Then ans_tokens has \"fifth-generation\" but the ans_text is \"generation\", which doesn't match.\n",
    "                ans_tokens = context_tokens[ans_start_wordloc:ans_end_wordloc+1]\n",
    "                if \"\".join(ans_tokens) != \"\".join(ans_text.split()):\n",
    "                    num_tokenprob += 1\n",
    "                    continue # skip this question/answer pair\n",
    "\n",
    "                examples.append((' '.join(context_tokens), ' '.join(question_tokens), ' '.join(ans_tokens), ' '.join([str(ans_start_wordloc), str(ans_end_wordloc)])))\n",
    "\n",
    "                num_exs += 1\n",
    "\n",
    "    print(\"Number of (context, question, answer) triples discarded due to char -> token mapping problems: \", num_mappingprob)\n",
    "    print(\"Number of (context, question, answer) triples discarded because character-based answer span is unaligned with tokenization: \", num_tokenprob)\n",
    "    print(\"Number of (context, question, answer) triples discarded due character span alignment problems (usually Unicode problems): \", num_spanalignprob)\n",
    "    print(\"Processed %i examples of total %i\\n\" % (num_exs, num_exs + num_mappingprob + num_tokenprob + num_spanalignprob))\n",
    "\n",
    "    # shuffle examples\n",
    "    indices = list(range(len(examples)))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    with open(os.path.join(out_dir, tier +'.context'), 'w', encoding = 'utf-8') as context_file,  \\\n",
    "         open(os.path.join(out_dir, tier +'.question'), 'w', encoding = 'utf-8') as question_file,\\\n",
    "         open(os.path.join(out_dir, tier +'.answer'), 'w', encoding = 'utf-8') as ans_text_file, \\\n",
    "         open(os.path.join(out_dir, tier +'.span'), 'w', encoding = 'utf-8') as span_file:\n",
    "\n",
    "        for i in indices:\n",
    "            (context, question, answer, answer_span) = examples[i]\n",
    "\n",
    "            # write tokenized data to file\n",
    "            write_to_file(context_file, context)\n",
    "            write_to_file(question_file, question)\n",
    "            write_to_file(ans_text_file, answer)\n",
    "            write_to_file(span_file, answer_span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data has 3062 examples total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing train: 100%|█████████████████████| 19/19 [00:01<00:00, 11.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of (context, question, answer) triples discarded due to char -> token mapping problems:  0\n",
      "Number of (context, question, answer) triples discarded because character-based answer span is unaligned with tokenization:  27\n",
      "Number of (context, question, answer) triples discarded due character span alignment problems (usually Unicode problems):  8\n",
      "Processed 3027 examples of total 3062\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    data_dir = \"C:/Users/shiva.m/Desktop/ML/Content ML/MultiModelData\"\n",
    "    train_filename = \"train-v.1.1.final_output.json\"\n",
    "    \n",
    "    # read train set\n",
    "    train_data = data_from_json(os.path.join(data_dir, train_filename))\n",
    "    print(\"Train data has %i examples total\" % total_exs(train_data))\n",
    "\n",
    "    # preprocess train set and write to file\n",
    "    preprocess_and_write(train_data, 'train', data_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
